# this resource sometimes require that a validating webhook be deleted
# when the chart is upgraded.
# https://github.com/prometheus-community/helm-charts/issues/108
# retries can be forced by changing maxRetries below. 0 is infinite.
---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  # rollback:
  #   enable: true
  #   maxRetries: 10
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 55.5.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  install:
    createNamespace: true
    crds: Create
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
  values:
    alertmanager:
      enabled: true
      config:
        global:
          resolve_timeout: 5m
          slack_api_url: "${SECRET_SLACK_WEBHOOK}"
        route:
          group_by: ["job"]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: "null"
          routes:
            - match:
                alertname: Watchdog
              receiver: "null"
            - receiver: "slack-notifications"
              continue: true
        receivers:
          - name: "null"
          - name: "slack-notifications"
            slack_configs:
              - channel: "#prometheus"
                title: |-
                  [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                  {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                    {{" "}}(
                    {{- with .CommonLabels.Remove .GroupLabels.Names }}
                      {{- range $index, $label := .SortedPairs -}}
                        {{ if $index }}, {{ end }}
                        {{- $label.Name }}="{{ $label.Value -}}"
                      {{- end }}
                    {{- end -}}
                    )
                  {{- end }}
                text: <-
                  {{ range .Alerts -}}
                  *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}

                  *Description:* {{ .Annotations.description }}

                  *Details:*
                  {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                  {{ end }}
                  {{ end }}
                send_resolved: true

      ingress:
        enabled: true
        pathType: Prefix
        annotations:
          kubernetes.io/ingress.class: "nginx"
          hajimari.io/enable: "false"
        hosts:
          - "alertmanager.${SECRET_DOMAIN}"
        tls:
          - hosts:
              - "alertmanager.${SECRET_DOMAIN}"
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Gi
    # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
    grafana:
      adminPassword: "${SECRET_GRAFANA_PASSWORD}"
      ingress:
        enabled: true
        pathType: Prefix
        annotations:
          kubernetes.io/ingress.class: "nginx"
          hajimari.io/appName: "Grafana"
          hajimari.io/icon: "simple-icons:grafana"
          hajimari.io/group: "monitoring"
          hajimari.io/enable: "true"
        hosts:
          - "grafana.${SECRET_DOMAIN}"
        tls:
          - hosts:
              - "grafana.${SECRET_DOMAIN}"
      plugins:
        - natel-discrete-panel
        - pr0ps-trackmap-panel
        - grafana-piechart-panel
        - grafana-worldmap-panel
        - grafana-clock-panel
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          # Ref: https://grafana.com/grafana/dashboards/11315
          unifi-client-insights:
            gnetId: 11315
            revision: 8
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11311
          unifi-network-sites:
            gnetId: 11311
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11314
          unifi-uap-insights:
            gnetId: 11314
            revision: 9
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11312
          #unifi-usw-insights:
          #  gnetId: 11312
          #  revision: 8
          #  datasource: Prometheus
          ## Ref: https://grafana.com/grafana/dashboards/11313
          #unifi-usg-insights:
          #  gnetId: 11313
          #  revision: 8
          #  datasource: Prometheus
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          #url: http://monitoring-loki:3100
          url: http://loki:3100
        #- name: HomeAssistant
        #  type: influxdb
        #  access: proxy
        #  url: http://influxdb:8086
        #  database: home_assistant
      sidecar:
        dashboards:
          enabled: true
        datasources:
          enabled: true
        kubeApiServer:
      enabled: true
    kubeApiServer:
      enabled: true
    kubeControllerManager:
      enabled: true
      endpoints:
        - 172.16.1.50
      # service:
      #   enabled: true
      #   port: 10257
      #   targetPort: 10257
    kubeScheduler:
      enabled: true
      endpoints:
        - 172.16.1.50
      # service:
      #   enabled: true
      #   port: 10251
      #   targetPort: 10251
    kubeProxy:
      enabled: true
      endpoints:
        - 172.16.1.50
      # service:
      #   enabled: true
      #   port: 10249
      #   targetPort: 10249
    kubeEtcd:
      enabled: false # turn this off until I get a chance to try a multi-master cluster.
      # endpoints:
      #   - 172.16.1.50
      # service:
      #   enabled: true
      #   port: 2381
      #   targetPort: 2381
    prometheus:
      ingress:
        enabled: true
        pathType: Prefix
        annotations:
          kubernetes.io/ingress.class: "nginx"
          hajimari.io/enable: "false"
        hosts:
          - "prometheus.${SECRET_DOMAIN}"
        tls:
          - hosts:
              - "prometheus.${SECRET_DOMAIN}"
    prometheusOperator:
      prometheusConfigReloader:
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 100Mi
      thanosService:
        enabled: false
      thanosServiceMonitor:
        enabled: false
      prometheusSpec:
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        #retention: 6h
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          # this is a hack:
          # https://github.com/helm/charts/issues/9288#issuecomment-711165029
          volumeClaimTemplate:
            spec:
              storageClassName: standard
              resources:
                requests:
                  storage: 10Gi
              selector: {}
        securityContext:
          runAsGroup: 2316
          runAsNonRoot: true
          runAsUser: 2316
          fsGroup: 2316
      admissionWebhooks:
        timeoutSeconds: 30
